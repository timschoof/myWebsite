[{"authors":["admin"],"categories":null,"content":"I am a Research Associate at the department of Speech, Hearing and Phonetic Sciences at UCL. My research primarily focuses on the question why older adults, with and without hearing loss, experience increased difficulties understanding speech in noise. I am particularly interested in the potential effects of cochlear synaptopathy, or \u0026lsquo;hidden hearing loss\u0026rsquo;, on speech perception. I have also looked at individual differences in hearing aid outcomes using different signal processing strategies. My aim is to ultimately be able to improve the listening experience of older adults. In addition to science, I absolutely love statistics and programming. When I am not working, I am either travelling the world, or planning a trip.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am a Research Associate at the department of Speech, Hearing and Phonetic Sciences at UCL. My research primarily focuses on the question why older adults, with and without hearing loss, experience increased difficulties understanding speech in noise. I am particularly interested in the potential effects of cochlear synaptopathy, or \u0026lsquo;hidden hearing loss\u0026rsquo;, on speech perception. I have also looked at individual differences in hearing aid outcomes using different signal processing strategies.","tags":null,"title":"Tim Schoof","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536447600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536447600,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00+01:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.\n  Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906545600,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":[],"content":"\rWelcome to the future of paper writing!! Ok, I’ll tone down my enthusiasm, but… it’s AWESOME!!!\nYou can write papers in RStudio and it’s super neat. So reproducible! It’s never been easier to embed your analyses in your manuscript. No more copy-pasting of results into a table, or trying to remember which bit of R code generated that one particular figure you’d like to tweak. It’s all in one place and if you tweak your analyses, the results are automatically updated in your manuscript. Magic.\nI’m sure there are lots of different ways to go about this successfully, but here’s what’s working well for me. I’ve started using the papaja (Preparing APA Journal Articles) R package. It’s basically a template to produce documents that follow APA manuscript guidelines. In other words, you can write something in R that’s publication ready!\n\rGetting started\rFirst, make sure you have the latest versions of R and RStudio installed – always a good idea.\nNext, it’s time to install the package! You can do this by typing the following into your RStudio command window:\n# Install devtools package if necessary\rif(!\u0026quot;devtools\u0026quot; %in% rownames(installed.packages())) install.packages(\u0026quot;devtools\u0026quot;)\r# Install the stable development verions from GitHub\rdevtools::install_github(\u0026quot;crsh/papaja\u0026quot;)\rThe installation procedure will probably ask you if you want to update / install a bunch of packages. You’ll need to update / install all the suggested packages, otherwise it won’t install papaja. So: select ‘All’. This didn’t actually work for me, but you might have more luck. Instead, I ended up installing all the required packages by hand: install.packages(“PackageName”). I then tried again. Eventually, it stopped suggesting packages I should update and it worked.\nYay, you managed to do the hardest bit, you installed the package!\n\r\rSetting up your RStudio\rWhile I have your attention, I’m going to suggest making some changes to your RStudio default settings that will help you be more reproducible.\nClick on Tools \u0026gt; Global Options. In the General tab, uncheck the following:\n\rRestore .RData into workspace at startup\rAlways save history (even when not saving .RData)\rRemove duplicate entries in history\r\rAnd set ‘Save workspace to .RData on exit’ to ‘Never’.\nYou’re just making sure that you’re no longer saving (and reloading) any objects and other things that you’ve create in your R session (but that isn’t actually in your scripts). This is especially important once you start sharing your code!\n\rOptimise your workflow\rWhenever you start a new research project, with a set of analyses and a reproducible manuscript, start a new RStudio Project. There are quite a few advantages to this. Mainly, it will resolve any file path problems, which is especially important if you want your code to run on someone else’s computer.\nTo create a new RStudio Project, click on file \u0026gt; new project \u0026gt; new directory \u0026gt; new project \u0026gt; name your project \u0026gt; create project.\nYou’ll now see that you’ve created a .Proj file, which is basically your project’s RStudio start-up file. Whenever you want to continue working on this project, just click on the .Proj file and RStudio will start a new session of R and will set the working directory to the project directory. It also restores any RStudio settings you may have specified.\nIt’s good practice to put everything that’s relevant to your project in this project folder, or at least your data and your R code. This not only makes it easier to write your reproducible document, it also makes it much easier to share your data and code with the scientific community in one go later on.\n\rWriting, at last!\rThe GitHub page for the papaja package explains exactly how to get started. There’s also a very handy user manual for the package. In short, however, to create a new R Markdown file using the papaja APA template, click on File \u0026gt; New File \u0026gt; R Markdown \u0026gt; From Template and select APA article (6th edition).\nNow just start typing :-)\n\rOf course writing a document in R is a little bit different from writing Word document. In fact, it’s more like writing a LaTeX document. Don’t let this freak you out, it’s a learning curve that you can handle!\nThe new RMarkdown file you have just created has a basic structure to it, which will help you get going. The first section (called the YAML), surrounded by —, contains metadata such as your paper’s title, author names, and abstract. This is also where you can tell RStudio to load any LaTeX packages that you want to use.\nRMarkdown and LaTeX\rTo get to grips with the basics of RMarkdown, check out the tutorial on the RStudio website. It’s quite short. There’s also a very handy cheat sheet for when you’ve forgotten how to create a new header or a numbered list, or how to use italics.\nWithin RMarkdown, especially if you’re generating a PDF file, you can also use LaTeX commands. This can be very helpful if you want to include a mathematical formula or Greek letters for example. This is just to say that if you’re trying something and it doesn’t work, you may want to add the word ‘LaTeX’ into your Google search terms.\n\rEmbedding R code\rThe main draw of writing your manuscripts in R is of course that you can easily embed R code. So how do you go about that?\nYou create something called a code chunk in your .rmd file, where you surround your R code with ```r and ``` as follows:\n```{r}\r# plot something\rplot(cars)\r```\rThe first line tells RMarkdown to expect some R code coming up. You can add lots of parameters in the curly brackets there, including figure captions, and instructions on whether to display the code or warning messages in your document. The cheat sheet shows you all the options.\nInstead of typing your R code in this chunk, you can also pull some code from an external script (preferably in the same project folder). You can do this using the read_chunk function. In your manuscript, create a code chunk that reads in this external script:\n```{r external, include = FALSE}\rknitr::read_chunk(\u0026quot;myScript.R\u0026quot;)\r```\rYou can then run subsections of this external script as follows:\n```{r plot-audiogram}\r```\rYou just need to name the different sections in your external R script so R knows exactly which bits you want to pull into your manuscript. Here I’m calling a code chunk called plot-audiogram from myScript.R. In myScript, the relevant code is indicated as follows:\n## ---- plot-audiogram\nSide note: the opposite of read_chunk is purl, which extracts all R code chunks from your RMarkdown document and puts it in a separate R file.\n\rGenerating output files\rTo turn your RMarkdown file (.rmd) into a PDF or LaTeX file, just click on ‘knit’. Also, all the figures you embeded into your manuscript will be saved as PDFs into a separate folder. Super convenient! Now just sit back, relax, and watch the error messages come in ;-)\nWhen the time comes to submit your manuscript, most journals will allow you to submit a LaTeX file. However, if your journal of choice only allows you to submit Word documents, don’t despair, you should be able to render Word documents too (although it’s less streamlined). And if not, there is an R package for that! I haven’t tried it yet, but check out the redoc package. It apparently generates Word documents that can be de-rendered back into R Markdown, retaining edits on the Word document, including tracked changes. Great for collaborating with people who might not be ready to hop on the R train.\n\r\rVersion control\rTo be extra cool, put everything in a GitHub repository! No, seriously, not just for the street cred, it’s actually useful. You can have them set to private if you’re at an educational institution.\nUseful resources\r\rPapaja package: https://github.com/crsh/papaja\rPapaja user manual with more details about writing papers in RStudio, including how to cite papers: https://crsh.github.io/papaja_man/introduction.html#getting-started\rRead more about workflows and RMarkdown in Nicholas Tierney’s short book about RMarkdown, which is specifically aimed at scientists.\rI encourage you to read this great blog post by Jenny Bryan.\rGet to grips with RMarkdown: https://rmarkdown.rstudio.com/lesson-1.html\rRMarkdown cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf\rredoc R package: https://github.com/noamross/redoc\r\r\r\r","date":1561593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561593600,"objectID":"322c6d34ee0afbb26ccce7f4a3b20f8a","permalink":"/post/reproducible-papers-r/","publishdate":"2019-06-27T00:00:00Z","relpermalink":"/post/reproducible-papers-r/","section":"post","summary":"Welcome to the future of paper writing!! Ok, I’ll tone down my enthusiasm, but… it’s AWESOME!!!\nYou can write papers in RStudio and it’s super neat. So reproducible! It’s never been easier to embed your analyses in your manuscript. No more copy-pasting of results into a table, or trying to remember which bit of R code generated that one particular figure you’d like to tweak. It’s all in one place and if you tweak your analyses, the results are automatically updated in your manuscript.","tags":[],"title":"Writing reproducible papers in R","type":"post"},{"authors":null,"categories":[],"content":"\rIf you’re involved in hearing science, chances are you’re plotting audiogram data for most of your publications. So why not write some code to make those plots and automate the process? Here I’ll walk you through some R code that you could use and/or adapt to plot your audiogram data.\nGetting ready\r\rFirst things first, let’s make sure you have all the require R packages installed and loaded.\nif (!require(here)) install.packages(\u0026quot;here\u0026quot;)\rif (!require(tidyverse)) install.packages(\u0026quot;tidyverse\u0026quot;)\rNext, let’s read in our data file. I’m going to assume that you have saved your data in a .csv file in the following format: one row per participant, with one column with participant IDs, perhaps another one indicating participant group, and the other columns indicating test ear and frequency. In other words, I’m assuming your data is in wide format.\ndata\u0026lt;-read.csv(here(paste(csvFile,\u0026quot;.csv\u0026quot;,sep=\u0026quot;\u0026quot;)),header=T)\r\r\rparticipant\rgroup\rR250\rR500\rR1000\rR2000\rR4000\rR8000\rL250\rL500\rL1000\rL2000\rL4000\rL8000\r\r\r\rP01\rpatient\r40\r40\r50\r55\r55\r95\r30\r30\r35\r40\r70\r80\r\rP02\rpatient\r40\r30\r40\r60\r65\r45\r25\r25\r30\r35\r65\r45\r\rP03\rpatient\r20\r20\r20\r35\r25\r85\r15\r25\r35\r55\r75\r80\r\r\r\rThe here package is pretty neat. It helps you avoid having to specify your working directory, and other file paths that are specific to your local computer. It makes the code more likely to work on someone else computer without them having to go through your code and changing all the hard-coded file paths that might be lurking around in there. Also, I don’t want Jenny Bryan to set my computer on fire, so there’s that.\n\rCleaning your data\r\rTo make things easier for ourselves when plotting the data, we’ll need to transform our data from wide to long format. We’ll want one column for participant IDs, perhaps another one indicating the participant group, test ear, test frequency, and audiometric threshold (in dB HL).\nif(\u0026quot;group\u0026quot; %in% colnames(data)){\rlong_data \u0026lt;- gather(data, key = \u0026quot;ear-freq\u0026quot;, value = \u0026quot;dB\u0026quot;,-participant,-group)\r} else {\rlong_data \u0026lt;- gather(data, key = \u0026quot;ear-freq\u0026quot;, value = \u0026quot;dB\u0026quot;,-participant)\r}\rHere I’m using the gather function to reshape our dataset from wide to long format. We’re leaving the participant and, if it exists, the group columns more or less untouched. All the other columns will be gathered up and the values will be dropped into the dB column and the column names (i.e. the keys) will be dropped into the ear-freq column. The output of all of this is assigned to a new variable, long_data.\n\r\rparticipant\rgroup\rear-freq\rdB\r\r\r\rP01\rpatient\rR250\r40\r\rP02\rpatient\rR250\r40\r\rP03\rpatient\rR250\r20\r\r\r\rWe still have a bit of cleaning up to do!\nd \u0026lt;- long_data %\u0026gt;% separate(col = \u0026quot;ear-freq\u0026quot;, into = c(\u0026quot;ear\u0026quot;,\u0026quot;freq\u0026quot;), sep = (1))\rHere I’m taking the long_data we just created and I’m “piping” or feeding it into the separate function. This line of code splits the “ear-freq” column into two separate columns, indicating the test ear and frequency. Note that here I’m assuming that the ear-freq column name in your data file can be split into two by simply taking the first character (sep = (1)) and separating it from the other characters in the column name. That will work in our example data set, where an example column name is R500, but won’t work if you’re using a different coding system (e.g. Right500 or 500R). You may need to use some fancy regular expressions in that case to split your columns.\n\r\rparticipant\rgroup\rear\rfreq\rdB\r\r\r\rP01\rpatient\rR\r250\r40\r\rP02\rpatient\rR\r250\r40\r\rP03\rpatient\rR\r250\r20\r\r\r\rLet’s extend this bit of code and continue cleaning.\nd \u0026lt;- long_data %\u0026gt;% separate(col = \u0026quot;ear-freq\u0026quot;, into = c(\u0026quot;ear\u0026quot;,\u0026quot;freq\u0026quot;), sep = (1)) %\u0026gt;%\rmutate(freq = (type.convert(freq))/1000) %\u0026gt;% mutate(freqLabels = formatC(freq, format=\u0026quot;g\u0026quot;)) \rNext, I’m ‘mutating’, or transforming, the data a bit further. I’m taking the values in the freq column and converting them to kHz by dividing the values by 1000. A funny thing happens. Our values have trailing zeros (e.g. 8.00 kHz). That would look a bit silly in our figure. So, let’s create a new column that contains the exact same values, but without any trailing zeros. The formatC function turns our numeric values into characters, so we’ll just use this column for our axis tick labels when we plot our data.\nThe only thing left to clean is our ear column. We’re simply turning this column into a factor and changing ‘R’ to ‘Right’ and ‘L’ to ‘Left’.\nd \u0026lt;- long_data %\u0026gt;% separate(col = \u0026quot;ear-freq\u0026quot;, into = c(\u0026quot;ear\u0026quot;,\u0026quot;freq\u0026quot;), sep = (1)) %\u0026gt;%\rmutate(freq = (type.convert(freq))/1000) %\u0026gt;% mutate(freqLabels = formatC(freq, format=\u0026quot;g\u0026quot;)) %\u0026gt;% mutate(ear = factor(ear, levels = c(\u0026quot;R\u0026quot;, \u0026quot;L\u0026quot;))) %\u0026gt;%\rmutate(ear = recode(ear, \u0026quot;R\u0026quot; = \u0026quot;Right\u0026quot;, \u0026quot;L\u0026quot; = \u0026quot;Left\u0026quot;)) \r\r\rparticipant\rgroup\rear\rfreq\rdB\rfreqLabels\r\r\r\rP01\rpatient\rRight\r0.25\r40\r0.25\r\rP02\rpatient\rRight\r0.25\r40\r0.25\r\rP03\rpatient\rRight\r0.25\r20\r0.25\r\r\r\r\rDifferent groups\rImagine you have an experimental group of, say, older adults with age-related hearing loss, and a control group of young university students. In this case, the hearing thresholds of the individuals in the control group are not necessarily all that interesting. Probably all you want to do is just plot the range of thresholds of the people you included. For the experimental group, on the other hand, I’d like to argue that it’s very important to show the thresholds of all individuals you included in the study, rather than just showing group means and standard deviations.\nSince we’ll be plotting the data for our experimental and control groups somewhat differently, it’s easiest to just create two different subsets of the data.\n\r# Experimental group\rpatient \u0026lt;- d %\u0026gt;%\rsubset(group %in% \u0026quot;patient\u0026quot;)\r# Control group\rcontrol \u0026lt;- d %\u0026gt;%\rsubset(group %in% \u0026quot;control\u0026quot;) %\u0026gt;% group_by(freq, ear) %\u0026gt;% summarize(mindB = min(dB), maxdB = max(dB)) %\u0026gt;% gather(key = \u0026quot;MinMax\u0026quot;, value = \u0026quot;dB\u0026quot;,-freq, -ear)\rFor the control group, we need to do a little bit more. We need to compute the minimum and maximum thresholds for each frequency to be able to plot the range. Let’s extend our code as follows:\n# Control group\rcontrol \u0026lt;- d %\u0026gt;%\rsubset(group %in% \u0026quot;control\u0026quot;) %\u0026gt;% group_by(freq, ear) %\u0026gt;% summarize(mindB = min(dB), maxdB = max(dB)) %\u0026gt;% gather(key = \u0026quot;MinMax\u0026quot;, value = \u0026quot;dB\u0026quot;,-freq, -ear)\rHere I am grouping the data by frequency and ear. It’s a little bit as if I’m taking subsets of the data, but it allows me to do the same computation on each of these subsets simultaneously. Next, I’m computing some summary statistics - the minimum and maximum - on the data for each ear and frequency combination separately. The last thing I need to do is make sure the data is in a long format, so we’ll use gather again here. We’ll end up with a frequency, ear, and dB column, as well as a min-max column that indicates whether the dB value is the minimum or maximum threshold within the group.\n\rPlotting\rFinally, we’re ready to plot some data!\n\rggplot()+\rfacet_grid(. ~ ear) +\rgeom_line(data = patient, aes(x=freq, y=dB, group=participant))+\rstat_summary(data = patient,\raes(x=freq, y=dB,group=ear), fun.y=mean, geom=\u0026quot;line\u0026quot;, lwd = 1.5)\rWe’ll be using ggplot to plot our data. This bit of code creates two subplots, one for each ear, using the facet_grid function. It then creates a line plot for the experimental group’s data (the ‘patient’ subset), with frequency along the x-axis, thresholds (dB HL) along the y-axis, and an individual line for each participant (group = participant). In addition, using the stat_summary function, we’re able to plot a thicker line (geom = line, lwd = 1.5) showing the mean for the group (fun.y = mean).\nNext, let’s add the control group’s data to that.\nggplot()+\rfacet_grid(. ~ ear) +\rgeom_line(data = patient, aes(x=freq, y=dB, group=participant))+\rstat_summary(data = patient,\raes(x=freq, y=dB,group=ear), fun.y=mean, geom=\u0026quot;line\u0026quot;, lwd = 1.5)+\rgeom_area(data = control, aes(x=freq, y=dB, group=MinMax),fill=\u0026quot;grey\u0026quot;, alpha=.7)\rTo create a shaded area, we’ll need to use the geom_area function. We’re specifying that we’re using the control group’s data. Our ‘group’ here corresponds to the ‘MinMax’ column, which just indicates whether the dB value is the upper or lower boundaries of our shaded area. The shaded area will be plotted in grey, with 70% transparency (alpha = .7).\nThe graph doesn’t quite look ready for publication…\nggplot()+\rfacet_grid(. ~ ear) +\rgeom_line(data = patient, aes(x=freq, y=dB, group=participant))+\rstat_summary(data = patient,\raes(x=freq, y=dB,group=ear), fun.y=mean, geom=\u0026quot;line\u0026quot;, lwd = 1.5)+ geom_area(data = control, aes(x=freq, y=dB, group=MinMax),\rfill=\u0026quot;grey\u0026quot;, alpha=.7)+ scale_y_reverse(limits = c(100,-10), breaks = seq(-10, 100, by=10))+\rscale_x_log10(breaks = unique(d$freq), labels = unique(d$freqLabels))+\rtheme_bw()\rThe scale_y_reverse function does what it says on the tin, it flips the y-axis so that negative values are up and positive values are down. I’ve set the axis limits to to -10 and 100, and the ticks (or breaks) from -10 to 100, every 10 dB. The scale_x_log10 function makes sure that our x-axis is on a log scale. I’m using the freq column to specify where the tick marks should. In addition I am using the freqLabels column to specify the labels for the tick marks (to show our kHz values without trailing zeros).\nFor comparison, here’s what it would look like without using the freqLabels column to specify the labels for the tick marks.\nAll that’s left to do now is add some axis labels and change the font size here and there.\nggplot()+\rfacet_grid(. ~ ear) +\rgeom_line(data = patient, aes(x=freq, y=dB, group=participant))+\rstat_summary(data = patient,\raes(x=freq, y=dB,group=ear), fun.y=mean, geom=\u0026quot;line\u0026quot;, lwd = 1.5)+ geom_area(data = control, aes(x=freq, y=dB, group=MinMax),\rfill=\u0026quot;grey\u0026quot;, alpha=.7)+ scale_y_reverse(limits = c(100,-10), breaks = seq(-10, 100, by=10))+\rscale_x_log10(breaks =unique(d$freq), labels = unique(d$freqLabels))+\rtheme_bw()+\rtheme(axis.text=element_text(size=12),\raxis.title=element_text(size=14),\rstrip.text.x = element_text(size = 12))+\rlabs(x = \u0026quot;Frequency (kHz)\u0026quot;, y = \u0026quot;Threshold (dB HL)\u0026quot;)\rYour audiogram figure is ready for publication!\nThe code (with some more practical instructions on how to run it) is available on my github page.\r\r\r","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"5496949ccce7d34496b5f2d6bcfa07ba","permalink":"/post/plotting-audiograms-in-r/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/post/plotting-audiograms-in-r/","section":"post","summary":"If you’re involved in hearing science, chances are you’re plotting audiogram data for most of your publications. So why not write some code to make those plots and automate the process? Here I’ll walk you through some R code that you could use and/or adapt to plot your audiogram data.\nGetting ready\r\rFirst things first, let’s make sure you have all the require R packages installed and loaded.","tags":[],"title":"Plotting audiograms in R","type":"post"},{"authors":["Axelle Calcus","**Tim Schoof**","Stuart Rosen","Barbara Shinn-Cunningham","Pamela Souza"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"1ed5b2a1e2fcd33ec5bd1f895c7a2f9c","permalink":"/publication/earhear2019b/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/publication/earhear2019b/","section":"publication","summary":"Objectives: This study aimed to evaluate the informational component of speech-on-speech masking. Speech perception in the presence of a competing talker involves not only informational masking, but a number of masking processes involving interaction of masker and target energy in the auditory periphery. Such peripherally generated masking can be eliminated by presenting the target and masker in opposite ears (dichotically). However, this also reduces informational masking by providing listeners with lateralization cues that support spatial release from masking. In tonal sequences, informational masking can be isolated by rapidly switching the lateralization of dichotic target and masker streams across the ears, presumably producing ambiguous spatial percepts that interfere with spatial release from masking. However, it is not clear if this technique works with speech materials. Design: Speech reception thresholds (SRTs) were measured in 17 young normal-hearing adults for sentences produced by a female talker in the presence of a competing male talker under three different conditions: diotic (target and masker in both ears), dichotic, and dichotic but switching the target and masker streams across the ears. Because switching rate and signal coherence were expected to influence the amount of IM observed, these two factors varied across conditions. When switches occurred, they were either at word boundaries or periodically (every 116 ms) and either with or without a brief gap (84 ms) at every switch point. In addition, SRTs were measured in a quiet condition to rule out audibility as a limiting factor. Results: SRTs were poorer for the four switching dichotic conditions than for the non-switching dichotic condition, but better than for the diotic condition. Periodic switches without gaps resulted in the worst SRTs compared to the other switch conditions, thus maximizing informational masking. Conclusions: These findings suggest that periodically switching the target and masker streams across the ears (without gaps) was the most efficient in disrupting spatial release from masking. Thus, this approach can be used in experiments that seek a relatively pure measure of informational masking, and could be readily extended to translational research.","tags":null,"title":"Switching streams across ears to evaluate informational masking of speech-on-speech","type":"publication"},{"authors":["Pamela Souza","Kathryn Arehart","**Tim Schoof**","Melinda Anderson","Dorina Strori","Lauren Balmert"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"4587ffbf88962214a7084f5e41e459c4","permalink":"/publication/earhear2019a/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/publication/earhear2019a/","section":"publication","summary":"Objectives. Previous work has suggested that individual characteristics, including amount of hearing loss, age, and working memory ability, may affect response to hearing aid signal processing.  The present study aims to extend work using metrics to quantify cumulative signal modifications under simulated conditions to real hearing aids worn in everyday listening environments.  Specifically, the goal was to determine whether individual factors such as working memory, age, and degree of hearing loss play a role in explaining how listeners respond to signal modifications caused by signal processing in real hearing aids, worn in the listener's everyday environment, over a period of time. Design. Participants were older adults (age range 54-90 years) with symmetrical mild-to-moderate sensorineural hearing loss.  We contrasted two distinct hearing aid fittings: one designated as mild signal processing and one as strong signal processing.  Forty-nine older adults were enrolled in the study and thirty-five participants had valid outcome data for both hearing aid fittings. The difference between the two settings related to the wide dynamic range compression (WDRC) and frequency compression features. Order of fittings was randomly assigned for each participant.  Each fitting was worn in the listener's everyday environments for approximately five weeks prior to outcome measurements. The trial was double blind, with neither the participant nor the tester aware of the specific fitting at the time of the outcome testing. Baseline measures included a full audiometric evaluation as well as working memory and spectral and temporal resolution.  The outcome was aided speech recognition in noise. Results. The two hearing aid fittings resulted in different amounts of signal modification, with significantly less modification for the mild signal processing fitting.  The effect of signal processing on speech intelligibility depended on an individual's age, working memory capacity, and degree of hearing loss. Adults who were older demonstrated progressively poorer speech recognition at high levels of signal modification.  Working memory interacted with signal processing, with individuals with lower working memory demonstrating low speech intelligibility in noise with both processing conditions, and individuals with higher working memory demonstrating better speech intelligibility in noise with the mild signal processing fitting.  Amount of hearing loss interacted with signal processing, but the effects were very small.  Individual spectral and temporal resolution did not contribute significantly to the variance in the speech intelligibility score. Conclusions. When the consequences of a specific set of hearing aid signal processing characteristics were quantified in terms of overall signal modification, there was a relationship between participant characteristics and recognition of speech at different levels of signal modification.  Because the hearing aid fittings used were constrained to specific fitting parameters that represent the extremes of the signal modification that might occur in clinical fittings, future work should focus on similar relationships with a wider range of signal processing parameters.","tags":null,"title":"Understanding variability in individual response to hearing aid signal processing in wearable hearing aids","type":"publication"},{"authors":["Guangting Mai","**Tim Schoof**","Peter Howell"],"categories":null,"content":"","date":1548633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548633600,"objectID":"ac67f0b2b3c1e5d14268ae0e13c425dc","permalink":"/publication/neuroimage2019/","publishdate":"2019-01-28T00:00:00Z","relpermalink":"/publication/neuroimage2019/","section":"publication","summary":"Phase-locked responses are vital for auditory perception and they may vary with participants' arousal state and age. Two phase-locked neural responses that reflect fine-grained acoustic properties of speech were examined in the current study: the frequency-following response (FFR) to the speech fundamental frequency (F0), which originates primarily from the auditory brainstem, and the theta-band phase-locked response (θ-PLV) to the speech envelope that originates from the auditory cortices. The ways these responses were affected by arousal in adults across a wide age-range (19–75 years) were examined. Extracts from electroencephalographic (EEG) responses to repeated syllables were classified into either high or low arousal state based on the occurrence of sleep spindles. The magnitudes of both FFRs and θ-PLVs were statistically greater in the high, than in the low, arousal state. The difference in θ-PLV between the two arousal states was significantly associated with sleep spindle density in the young, but not the older, adults. The results show that (1) arousal affects phase-locked processing of speech at cortical/sub-cortical sensory levels; and that (2) there is an interplay between aging and arousal state which indicates that sleep spindles have an age-dependent neuro-regulatory role on cortical processes. The results lay the grounds for studying how cognitive states affect early-stage neural activity in the auditory system across the lifespan.","tags":null,"title":"Modulation of phase-locked neural responses to speech during different arousal states is age-dependent","type":"publication"},{"authors":["Melinda Anderson","Varsha Rallapalli","**Tim Schoof**","Pamela Souza","Kathryn Arehart"],"categories":null,"content":"","date":1541116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541116800,"objectID":"2fefc214c9967d4f4cffcb9beacd62f2","permalink":"/publication/ija2018/","publishdate":"2018-11-02T00:00:00Z","relpermalink":"/publication/ija2018/","section":"publication","summary":"Clinicians have long used self-report methods to assess hearing aid benefit. However, there are fewer data as to whether self-report instruments can be used to compare differences between signal processing settings. This study examined how self-perceived performance varied as a function of modifications in signal processing using two self-report measures. Data were collected as part of a double-blind randomised crossover clinical trial. Participants were fit with two fittings: mild processing (slow time constants, disabled frequency lowering) and strong processing (fast time constants, frequency lowering enabled). The speech, spatial, and qualities of hearing (SSQ) questionnaire and the Effectiveness of Auditory Rehabilitation (EAR) questionnaire were collected at multiple time points. Older adults with sensorineural hearing loss who had not used hearing aids within the previous year participated (49 older adults were consented; 40 were included in the final data analyses). Findings show that listeners report a difference in perceived performance when hearing aid features are modified. Both self-report measures were able to capture this change in perceived performance. Self-report measures provide a tool for capturing changes in perceived performance when hearing aid processing features are modified and may enhance provision of an evidence-based hearing aid fitting.","tags":null,"title":"The use of self-report measures to examine changes in perception in response to fittings using different signal processing parameters","type":"publication"},{"authors":["**Tim Schoof**","Pamela Souza"],"categories":null,"content":"","date":1538348400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538348400,"objectID":"3c486762812ce4a803b4e93822641fb2","permalink":"/publication/psyarxiv2018/","publishdate":"2018-10-01T00:00:00+01:00","relpermalink":"/publication/psyarxiv2018/","section":"publication","summary":"Objective: Older hearing-impaired adults typically experience difficulties understanding speech in noise. Most hearing aids address this issue using digital noise reduction. While noise reduction does not necessarily improve speech recognition, it may reduce the resources required to process the speech signal. Those available resources may, in turn, aid the ability to perform another task while listening to speech (ie, multitasking). This study examined to what extent changing the strength of digital noise reduction in hearing aids affects the ability to multitask. Design: Multitasking was measured using a dual-task paradigm, combining a speech recognition task and a visual monitoring task. The speech recognition task involved sentence recognition in the presence of six-talker babble at signal-to-noise ratios (SNRs) of 2 and 7 dB. Participants were fit with commercially-available hearing aids programmed under three noise reduction settings: off, mild, strong. Study sample: 18 hearing-impaired older adults. Results: There were no effects of noise reduction on the ability to multitask, or on the ability to recognize speech in noise. Conclusions: Adjustment of noise reduction settings in the clinic may not invariably improve performance for some tasks.","tags":null,"title":"Multitasking with typical use of hearing aid noise reduction in older listeners","type":"publication"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536447600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536447600,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+01:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Pamela Souza","**Tim Schoof**","Jing Shen"],"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"ab28320f87c433a2a3a04861cf42a036","permalink":"/publication/audiologytoday2017/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/publication/audiologytoday2017/","section":"publication","summary":"Older adults are living longer and more active lives than ever before. For audiologists, that means a larger number of clients who present with both auditory and cognitive changes. As researchers study the role of cognition in listening, what do clinicians need to know to guide treatment decisions?","tags":null,"title":"Can individual cognitive abilities direct audiology treatment?","type":"publication"},{"authors":null,"categories":[],"content":"\rThis work was presented as a poster at the American Auditory Society’s Annual Scientific and Technology Conference, 2-4 March 2017, Scottsdale, AZ. You can download a copy of the poster here.\nThe National Institutes of Health have recently undertaken steps to enhance rigor and reproducibility of biomedical research. For example, grant applications are now required to explicitly address how the proposed research will achieve robust and unbiased results. Reproducibility is an integral part of science as it allows for research findings to be either validated or refuted and lets scientists successfully build upon previous research. Only when multiple studies, conducted by different scientists, demonstrate similar results can we obtain a reasonable approximation of how the world works. In this blog post I will review some useful tools and discuss some good practices that support transparency and reproducibility.\nTo ensure the reproducibility of your research, you just need to follow these five simple rules:\nPre-register your study\rDocument everything you do\rDon’t do anything by hand, script everything\rUse a version control system\rProvide open access to all publications, scripts, and data\r\rFollowing these five rules will not only help other researchers validate or refute your findings, it will also make your research less error-prone, enhance your efficiency and productivity, and make your collaborations more effective. Moreover, it sends a clear signal of transparency and trustworthiness.\nSo how should we go about following these rules? In the sections below I will discuss some good practices and provide some useful tools that will help enhance the transparency and reproducibility of your research. I will also point you to some useful training resources.\nPre-register your study\rA few years ago, a group of scientists wrote an open letter to the Guardian newspaper advocating for study pre-registration in scientific publishing. The idea is that your study design and analysis details are peer reviewed before you start data collection. This helps distinguish exploratory from confirmatory analyses, guards against p-hacking or cherry-picking of data and analyses, and overcomes publication bias.\nFor a list of journals that currently offer pre-registration, have a look here. Most hearing science journals do not yet offer pre-registration. Let’s convince them!\nEven if your journal of choice does not give you the option to submit your study design and analysis plan for peer review, you can still create a registered report via:\n\rCenter for Open Science\rAsPredicted\rClinicalTrials.gov\r\r\rDocument, document, document\rIt is incredibly important to carefully document every step of the research process. Keep track of precisely why, when, and how you conducted your research. Instead of using a paper notebook or a collection of word documents, use an electronic lab notebook. This allows you to store all your research output in one place, access your notebook wherever you are, search entries, and share your notes with your collaborators.\nThere are a bunch of different electronic lab notebooks out there. Some are free, others are quite costly. Some allow you to store large volumes of data, others are more basic. Find one that best suits your needs. Some electronic lab notebooks that may be worth exploring (in no particular order) are:\n\rOpen Science Framework\rSlack\rSciNote\rMicrosoft OneNote\rEvernote\rLabii\rLabArchives\reLabJournal\rLabGuru\r\rIt is also incredibly important to provide detailed documentation for your scripts and your data. If either you or someone else ever wants to re-use it, they’ll need all the help they can get to make sense of your scripts and data files.\n\rAvoid manual data processing\rIn order for research to be fully reproducible, you have to script absolutely everything. If you manipulate your data by hand at any point, it is difficult to replicate exactly what you did at a later stage. And it may be practically impossible for anyone else to reproduce your results.\nUse R (instead of SPSS) for your statistical analyses. You can download both R and RStudio - a graphical user interface for R - for free. Software Carpentry offers a short course that walks you through the basics of Bash. Interested in learning Python? There are a lot of courses and books out there, such as Learn Python the hard way or MIT’s Introduction to computer science and programming using Python.\nIn general, some great websites that provide online programming courses are:\n\rCoursera\rSoftware Carpentry\redX\rLynda (check if your university has a subscription)\r\rFeeling tired just thinking about the idea of having to spend hours working your way through an online course? The best way to learn is just to get your hands dirty. So for your next data analysis, just open up RStudio, or the Bash Shell and figure things out as you go along. As always, Google is your friend, as is stackoverflow.\n\rAutomatically embed stats in your manuscripts\rThe next step in making your research reproducible, is to integrate your data, code, and manuscript. That way you can trace back the results you report in your paper to the underlying code and data. Automatically embedding your statistical output in your manuscripts will avoid the risk of any copy-paste errors when reporting your results. Another advantage is that you can automatically update your graphs and statistics after you have re-analysed your data.\nThere are some nifty solutions out there to automatically embed statistical output in your manuscripts. For example, if you are a Microsoft Word user, you can download StatTag, a free plug-in that allows you to embed statistics, tables, and figures into Word and update your results with one button press. Alternatively, you can write dynamic documents directly from RStudio, using R Markdown (which in turn uses the knitr and pandoc R packages). Have a look at some of the things that are possible with R Markdown here.\nGetting started with R Markdown is fairly straightforward. Here are some useful resources to help you get started:\n\rswcarpentry.github.io/r-novice-gapminder/15-knitr-markdown\rwww.coursera.org/learn/reproducible-research\rR Markdown cheat sheet\r\rFormatting is of course very important when preparing manuscripts for publication. Instead of using Microsoft Word, you could use LaTeX and integrate your statistical output using Sweave and knitr. However, even if you’re not a LaTeX user, you can prepare nicely formatted manuscripts in RStudio. For information, have a look at this tutorial and consider using the rticles R package.\nIn addition to generating reports and manuscripts, it is possible to create interactive web applications in RStudio using shiny. You can show your data on the web and allow people to play with a set of parameters to see what effect they have on your results. For some fancy examples, have a look at this showcase.\n\rUse a version control system\rHave you ever had multiple versions of a certain file floating around on your computer, having forgotten which version was the ‘right’ one? This can be particularly problematic if months or years down the line someone asks if you’d be happy to share the code you used in your paper. The solution to this issue is to use a version control system, such as Git. You can use it to keep track of changes in your documents and scripts by storing revisions of your files in a centralized repository. Using a version control system you can easily compare, restore, and merge different versions of files. What is particularly nice is that you can undo changes you made to a given script and its dependent files simultaneously. Rather than having to try to remember which files were changed in conjunction with one another, you can undo the changes to these files all at once.\nTo start using Git as your version control system, you need to do two things:\n\rInstall Git on your computer\rCreate an account with GitHub or GitLab\r\rGit itself serves as the local version control system on your computer. GitHub and GitLab provide centralized repositories where you can manage and share your files with your collaborators or the world.\nRepositories on GitHub are in principle public, which means that anyone can look at and download your code (people can’t make changes to your code unless they have your permission). If you are not ready to share your work, you can make your repository private. Usually you have to pay to host private repositories on GitHub. However, if you have a university email address, you can request an educational discount and have as many private repositories as you want, for free. Instead of using GitHub, you can also create an account with GitLab. You can have an unlimited number of private repositories on their site.\nThere is a bit of a learning curve to getting started with Git, but it is well worth it! Here are some useful resources to get you on your way:\n\rSoftware Carpentry intro to Git\rGit documentation\rGitHub user guides\r\r\rPublish open access\rThere has been a recent push for open access publishing. An increasing number of funding agencies, such as the NIH, the Research Councils UK, and the Wellcome Trust have established policies for open access publishing.\nThere are many advantages to making your publications publicly available. It drives innovation on a global scale, gives clinicians access to the latest research findings, and gives you increased visibility and citations. For an excellent overview of why open access publishing is so important, have a look at this video rom “Piled Higher and Deeper”, by Jorge Cham (phdcomics.com).\nThere are a few different options to provide open access to your publications.\nPay an article processing charge\rDeposit paper in an open access repository after an embargo period\rSubmit the post-print version of the paper to an online repository\rPost PDFs of your publications on your personal website\r\r\rProvide open access to your scripts\rAnd important part of enhancing the transparency and reproducibility of your research is to share your code, along with some (example) data and documentation. This enables others to exactly replicate your analyses. Your code can then also be used in future research. It may even lead to some citations!\nYou can of course share your code and supporting documents as a bunch of individual files. However, a nice way to distribute your code is to create an R package to go along with your publication. Everything will be nicely packaged up together. All people need to do is install your package and they can play with your code and data in R. You can create an R package in RStudio using the devtools package. For some more information on how to go about it, have a look at this tutorial.\nIf you’re using a version control system, like GitHub, the easiest way to share your scripts or R package is to link to your GitHub repository in your publication. You can even make your repository citable by assigning it a doi. You can learn more about how to do that here.\nIt might even be worth writing a paper specifically about your analysis methods and code in journals, such as:\n\rJournal of Open Research Software\rThe Journal of Open Source Software\rThe R Journal\r\r\rProvide open access to your data\rLast but not least, in addition to making your publications and code open access, it is important to provide open access to your de-identified data. This enhances the transparency of your research, helps preserve your data for the future, and enables reuse (including meta-analysis) of your data. It is of course important to keep HIPAA compliance and any intellectual property rights in mind when deciding how to go about sharing your data.\nWhen you archive your data, it is important to include detailed documentation so that you and others can make sense of all the different parts of your data set. In a few months or years, it is quite likely that you won’t remember what all the different fields in your excel file refer to.\nDeposit your data and documentation in an online repository, such as:\n\rHarvard Data Verse\rfigshare\rDryad Digital Repository\rOpen Science Framework\rDat Project\rA repository hosted by your institution, funder, or journal\r\rAcknowledgments: Many thanks to Pamela Souza for making this work possible. Thanks to José Joaquín Atria and Sriram Boothalingam for inspiring discussions on the topic. Work supported by NIH. \n\r","date":1487721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487721600,"objectID":"9b567939c5f61acbdc679e3dfc512b47","permalink":"/post/enhancing-transparency-and-reproducibility-of-hearing-science/","publishdate":"2017-02-22T00:00:00Z","relpermalink":"/post/enhancing-transparency-and-reproducibility-of-hearing-science/","section":"post","summary":"This work was presented as a poster at the American Auditory Society’s Annual Scientific and Technology Conference, 2-4 March 2017, Scottsdale, AZ. You can download a copy of the poster here.\nThe National Institutes of Health have recently undertaken steps to enhance rigor and reproducibility of biomedical research. For example, grant applications are now required to explicitly address how the proposed research will achieve robust and unbiased results. Reproducibility is an integral part of science as it allows for research findings to be either validated or refuted and lets scientists successfully build upon previous research.","tags":[],"title":"Enhancing transparency and reproducibility of hearing science","type":"post"},{"authors":null,"categories":null,"content":"My postdoctoral work at UCL looks at the potential implications of hidden hearing loss. Hidden hearing loss, or cochlear synaptopathy, refers to damage to the synapses connecting the inner hair cells in the cochlea to the auditory nerve. Crucially, this type of damage does not necessarily result in any changes in audiometric thresholds. Animal studies have shown that this damage can arise as a consequence of noise exposure or ageing. While evidence for hidden hearing loss in humans is emerging, it remains challenging to measure non-invasively. In addition, its consequences for auditory processing remain unclear. In particular, we are investigating the effects of hidden hearing loss on auditory temporal processing and speech perception in noise.\nI am also working on a project with Dr Sriram Boothalingam at the University of Wisconsin-Madison examining whether the auditory efferent system may play a protective role against hidden hearing loss.\n","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"b0c24893c35395d5612504455d008492","permalink":"/project/hhl/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/project/hhl/","section":"project","summary":"Postdoc at UCL 2017-2019","tags":null,"title":"Consequences of hidden hearing loss","type":"project"},{"authors":["**Tim Schoof**","Stuart Rosen"],"categories":null,"content":"","date":1475276400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475276400,"objectID":"d917f8db24c0990b82d43a7215fddf3f","permalink":"/publication/jaro2016/","publishdate":"2016-10-01T00:00:00+01:00","relpermalink":"/publication/jaro2016/","section":"publication","summary":"Older adults, even those without hearing impairment, often experience increased difficulties understanding speech in the presence of background noise. This study examined the role of age-related declines in subcortical auditory processing in the perception of speech in different types of background noise. Participants included normal-hearing young (19–29 years) and older (60–72 years) adults. Normal hearing was defined as pure-tone thresholds of 25 dB HL or better at octave frequencies from 0.25 to 4 kHz in both ears and at 6 kHz in at least one ear. Speech reception thresholds (SRTs) to sentences were measured in steady-state (SS) and 10-Hz amplitude-modulated (AM) speech-shaped noise, as well as two-talker babble. In addition, click-evoked auditory brainstem responses (ABRs) and envelope following responses (EFRs) in response to the vowel /ɑ/ in quiet, SS, and AM noise were measured. Of primary interest was the relationship between the SRTs and EFRs. SRTs were significantly higher (i.e., worse) by about 1.5 dB for older adults in two-talker babble but not in AM and SS noise. In addition, the EFRs of the older adults were less robust compared to the younger participants in quiet, AM, and SS noise. Both young and older adults showed a “neural masking release,” indicated by a more robust EFR at the trough compared to the peak of the AM masker. The amount of neural masking release did not differ between the two age groups. Variability in SRTs was best accounted for by audiometric thresholds (pure-tone average across 0.5–4 kHz) and not by the EFR in quiet or noise. Aging is thus associated with a degradation of the EFR, both in quiet and noise. However, these declines in subcortical neural speech encoding are not necessarily associated with impaired perception of speech in noise, as measured by the SRT, in normal-hearing older adults.","tags":null,"title":"The role of age-related declines in subcortical auditory processing in speech perception in noise","type":"publication"},{"authors":["**Tim Schoof**","Stuart Rosen"],"categories":null,"content":"","date":1441062000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441062000,"objectID":"13af1eeac12212eaf6f0b76ba1a95513","permalink":"/publication/jasa-el2015/","publishdate":"2015-09-01T00:00:00+01:00","relpermalink":"/publication/jasa-el2015/","section":"publication","summary":"This study examined the effects of sentence predictability and masker modulation type on the fluctuating masker benefit (FMB), the improvement in speech reception thresholds resulting from fluctuations imposed on a steady-state masker. Square-wave modulations resulted in a larger FMB than sinusoidal ones. FMBs were also larger for high compared to low-predictability sentences, indicating that high sentence predictability increases the benefits from glimpses of the target speech in the dips of the fluctuating masker. In addition, sentence predictability appears to have a greater effect on sentence intelligibility when the masker is fluctuating than when it is steady-state.","tags":null,"title":"High sentence predictability increases the fluctuating masker benefit","type":"publication"},{"authors":null,"categories":null,"content":"My postdoctoral work at Northwestern University focused on understanding individual differences that predict hearing aid outcomes, including speech in noise perception, listening effort, and perceived sound quality. Previous laboratory-based studies suggest a link between an individual\u0026rsquo;s susceptibility to signal modification caused by hearing aid processing and cognitive function. I worked on a clinical trial, using commercially available hearing aids, that investigated the effects of signal modification introduced by wide-dynamic range compression speed and frequency compression. We found that hearing aid outcomes relate to individual differences in audiometric thresholds, age, and working memory capacity.\n","date":1430434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430434800,"objectID":"ab2073e275dd86789ff3ead9ccb635db","permalink":"/project/has/","publishdate":"2015-05-01T00:00:00+01:00","relpermalink":"/project/has/","section":"project","summary":"Postdoc at Northwestern University 2015-2016","tags":null,"title":"Individual differences in hearing aid outcomes","type":"project"},{"authors":["**Tim Schoof**","Stuart Rosen"],"categories":null,"content":"","date":1415750400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1415750400,"objectID":"bc1d86a0bdeabe6c7acc06ab59af5c26","permalink":"/publication/frontiers2014/","publishdate":"2014-11-12T00:00:00Z","relpermalink":"/publication/frontiers2014/","section":"publication","summary":"Normal-hearing older adults often experience increased difficulties understanding speech in noise. In addition, they benefit less from amplitude fluctuations in the masker. These difficulties may be attributed to an age-related auditory temporal processing deficit. However, a decline in cognitive processing likely also plays an important role. This study examined the relative contribution of declines in both auditory and cognitive processing to the speech in noise performance in older adults. Participants included older (60–72 years) and younger (19–29 years) adults with normal hearing. Speech reception thresholds (SRTs) were measured for sentences in steady-state speech-shaped noise (SS), 10-Hz sinusoidally amplitude-modulated speech-shaped noise (AM), and two-talker babble. In addition, auditory temporal processing abilities were assessed by measuring thresholds for gap, amplitude-modulation, and frequency-modulation detection. Measures of processing speed, attention, working memory, Text Reception Threshold (a visual analog of the SRT), and reading ability were also obtained. Of primary interest was the extent to which the various measures correlate with listeners' abilities to perceive speech in noise. SRTs were significantly worse for older adults in the presence of two-talker babble but not SS and AM noise. In addition, older adults showed some cognitive processing declines (working memory and processing speed) although no declines in auditory temporal processing. However, working memory and processing speed did not correlate significantly with SRTs in babble. Despite declines in cognitive processing, normal-hearing older adults do not necessarily have problems understanding speech in noise as SRTs in SS and AM noise did not differ significantly between the two groups. Moreover, while older adults had higher SRTs in two-talker babble, this could not be explained by age-related cognitive declines in working memory or processing speed.","tags":null,"title":"The role of auditory and cognitive factors in understanding speech in noise by normal-hearing older listeners","type":"publication"},{"authors":["**Tim Schoof**","Tim Green","Andrew Faulkner","Stuart Rosen"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"077334f8cd40548afe99ce0f78a27ca4","permalink":"/publication/jasa2013/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/jasa2013/","section":"publication","summary":"Acoustic simulations were used to study the contributions of spatial hearing that may arise from combining a cochlear implant with either a second implant or contralateral residual low-frequency acoustic hearing. Speech reception thresholds (SRTs) were measured in twenty-talker babble. Spatial separation of speech and noise was simulated using a spherical head model. While low-frequency acoustic information contralateral to the implant simulation produced substantially better SRTs there was no effect of spatial cues on SRT, even when interaural differences were artificially enhanced. Simulated bilateral implants showed a significant head shadow effect, but no binaural unmasking based on interaural time differences, and weak, inconsistent overall spatial release from masking. There was also a small but significant non-spatial summation effect. It appears that typical cochlear implant speech processing strategies may substantially reduce the utility of spatial cues, even in the absence of degraded neural processing arising from auditory deprivation.","tags":null,"title":"Advantages from bilateral hearing in speech perception in noise with simulated cochlear implants and residual acoustic hearing","type":"publication"},{"authors":null,"categories":null,"content":"My PhD research addressed the question why older adults with normal hearing experience increased difficulties understanding speech in the presence of background noise. I looked at the relative contributions of age-related declines in subcortical auditory processing, as measured by the auditory brainstem response (ABR) and the frequency following response (FFR), and declines in cognitive processing. In short, the data suggested that age-related declines in auditory and cognitive processing, in the absence of hearing impairment, do not necessarily lead to increased speech in noise difficulties.\n","date":1314831600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1314831600,"objectID":"c9a66b2d3f935438f991c02f81127b35","permalink":"/project/phd/","publishdate":"2011-09-01T00:00:00+01:00","relpermalink":"/project/phd/","section":"project","summary":"PhD at UCL 2011 - 2014","tags":null,"title":"Effects of ageing on speech perception in noise","type":"project"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]